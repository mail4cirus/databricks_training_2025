{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d50c2eb-d0b2-447b-a1f5-ee60045579b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "databricks workflow jobs\n",
    "  bascially azure datafactory helps to automate all the tasks \n",
    "  task orchestration \n",
    "\n",
    "\n",
    "optimized storage\n",
    "  Delta lake\n",
    "  parquet\n",
    "  ice berg\n",
    "\n",
    "Unity Catalog\n",
    "  is a centralized data catalog that provides access control, auditing, data lineage, quality monitoring and data discovery across Databricks workspaces.\n",
    "\n",
    "Lake Flow\n",
    "  Connect :- A set of efficient ingestion connectors that simplify data ingestion from popular applications,database,cloud storage, message buses and local files\n",
    "  \n",
    "  Declarative pipelines :- A framework for building batch and streaming data pipelines using sql and python, desgined to accelerate ETL development.\n",
    "  \n",
    "  Jobs :- A workflow automation tool for Databricks that orchestrates data processing workloads.\n",
    "\n",
    "\n",
    "\n",
    "Data ingestion with LakeFlow connect\n",
    "  Data ingestion is streamlined with simple, efficient connectors that enable you to bring in data from files, cloud storage, databases, enterprise applications and streaming sources directly inyo Data bricks Lakehouse- all within a unified, managed platform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Data ingestion from cloud\n",
    "  Goal is to convert raw files from cloud into delta tables in databricks\n",
    "  Ingestion from cloud object storage is performed using Lakeflow connect standard connectors\n",
    "\n",
    "  1.create table as (ctas)(batch read) :- creates a delta table by default from the files stored in cloud object.\n",
    "      read_files :- \n",
    "        1. function is used to read files from a specified location and return the data in a tabular format.\n",
    "        2. automatically detcts the file format and infers a unified schema across all files.\n",
    "        3. allows you to specify format-specify options for greater control when reading source files.\n",
    "        4. can be used in streaming tables to incrementally ingest files into delta lake using auto loader.\n",
    "  \n",
    "    format below\n",
    "    create table new_table_name as \n",
    "      select * from \n",
    "        read_files(<path_to_files>,\n",
    "                    format => '<file_type>',\n",
    "                    <other_format_specific_options>\n",
    "        );\n",
    "\n",
    "  2.copy into(incremental load)\n",
    "      This command performs a bulk load from files in cloud object storage into the table, in the below example, it will load files into the empty table new_table. the FROM clause specifies the location of the csv files.\n",
    "      Create an empty table to copy data into.\n",
    "        create an empty table without a schema/ explicity create the table with a schema\n",
    "      \n",
    "      COPY INTO (legacy)\n",
    "      ******************\n",
    "      CREATE TABLE new_table;\n",
    "\n",
    "      COPY INTO new_table\n",
    "      FROM '<dir_path>'\n",
    "      FILEFORMAT=<file_type>\n",
    "      FORMAT_OPTIONS(<options>)\n",
    "      COPY_OPTIONS(<options>)\n",
    "\n",
    "\n",
    "      1.is a retriable and idempotent operation and will skip files that have already been loaded(incremental)\n",
    "      2.supports various common files types like parquet, json, xml etc.\n",
    "      3.FROM specifies the path of the cloud storage location continuously adding files\n",
    "      4.FORMAT_OPTIONS() controls how the source files are parsed and interpreted,\n",
    "      5.COPY_OPTIONS() controls the behviour of the COPY INTO operation itself, such as schema ec=vloution(mergeSchema) and idemptency (force)\n",
    "\n",
    "\n",
    "  3.Auto loader\n",
    "      1.incrementally and efficiently processes new data files(in batch or streaming) as they arrive in cloud storage without any additional setup\n",
    "      2.can use auto loader to process billions of files\n",
    "      3.Auto loader is built upon Spark Structured Streaming\n",
    "    \n",
    "    Python Auto loader\n",
    "\n",
    "      spark\n",
    "        .readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\",\"json\")\n",
    "          .option(\"cloudFiles.schemaLocation\",\"<checkpoint_path>\")\n",
    "          .load(\"/Volumes/catalog/schema/files\")\n",
    "        .writeStream\n",
    "          .option(\"checkpointLocation\",\"<checkpoint_path>\")\n",
    "          .trigger(processingTime=\"5 seconds\")\n",
    "          .toTable(\"catalog.database.table\")\n",
    "    \n",
    "    SQL Auto loader\n",
    "\n",
    "      CREATE or REFRESH STREAMING TABLE\n",
    "      catalog.schema.table SCHEDULE EVERY 1 HOUR\n",
    "      AS\n",
    "      SELECT * \n",
    "      FROM STREAM read_files(\n",
    "        '<dir_path>',\n",
    "        format => '<file_type>'\n",
    "      )\n",
    "\n",
    "\n",
    "\n",
    "    Refresh Streaming table(manually refresh table)\n",
    "      REFRESH STREAMING TABLE catalog.schema.table;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Rescued Data Column\n",
    "  captures mismatched or unparsable fields as json. preserving non-conforming input values in your lakehouse tables instead of dropping them.\n",
    "\n",
    "  read_files(),spark.read or aut loader provides a rescued data column if the raw data doesnt not match the schema.\n",
    "\n",
    "\n",
    "\n",
    "Lakeflow Connect(managed connectors)\n",
    "***********************************\n",
    "    data ingestion from wide variety of enterprise databases\n",
    "\n",
    "    lakeflow connect collects data from external sources to streaming delta tables using a serverless compute declarative pipelines\n",
    "\n",
    "    pipeline (Sas or api)\n",
    "      1. A lakeflow serverless declarative pipeline job collects credentials from unity catalog\n",
    "      2. the job reaches out to the publicly accessible data source(eg. API, open OLAP port etc..)\n",
    "      3. The service transforms the data and stores it to a streaming delta table\n",
    "\n",
    "\n",
    "    Lakeflow connect collects data from external databases to streaming delta tables\n",
    "      1. The classic compute declartive pipelines job collects credentials from unity catalog(uc)\n",
    "      2. It uses the credentials to connect and collect data from your databases surces\n",
    "      3. The latest state and staging data are saved to your unity catalog volume\n",
    "      4. A serverless declartive pipeline job process the collected data to your streaming delta table\n",
    "\n",
    "\n",
    "      A dedicated pipeline that connects to the database to extract\n",
    "        metadata\n",
    "        snapshots\n",
    "        change logs - it stages all of this in the unity catalog(UC) volume\n",
    "      \n",
    "      unity catalog volume:\n",
    "        This acts as the intermediate staging layer, enabling the next piepline to pick up and stream data \n",
    "        Its secured using standard UC mechanics, and by default, access is liited to the user running pipeline\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ingesting into existing delta table\n",
    "      MERGE INTO target_table target\n",
    "      USING source_table source\n",
    "      on target.id = source.id\n",
    "\n",
    "\n",
    "\n",
    "      update\n",
    "      \n",
    "      MERGE INTO target_table target\n",
    "      USING source_table source\n",
    "      on target.id = source.id\n",
    "      WHEN MATCHED AND source.status = 'update' THEN\n",
    "        UPDATE SET\n",
    "          target.email = source.email,\n",
    "          target.status = source.status\n",
    "      WHEN MATCHED AND source.status = 'delete' THEN\n",
    "        DELETE\n",
    "      WHEN NOT MATCHED THEN\n",
    "        INSERT (id,first_name,email,sign_up_date,status)\n",
    "        VALUES(source.id,source.first_name,source.email,source.sign_up_date,source.status);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6e4a86c-b917-4408-88eb-dc7e039121b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Jobs\n",
    "  primary resource for \n",
    "    1. Scheduling\n",
    "    2. Coordinating\n",
    "    3. Running operations such as data processing, ETL, analytics and machine learning workloads within databricks environment\n",
    "\n",
    "task\n",
    "  single unit of work wirhin a job that executes a specific workload suck as notebook, script, query and more\n",
    "\n",
    "\n",
    "Each job consists of one or more tasks, which are the individual units of work that make up the job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee41623-db33-428c-9c3b-b5c55620c3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Change data capture (CDC)\n",
    "  is a tecnique used to track and capture changes in a data source (such as database, lakehouse or data warehouse)\n",
    "  Those changes are then applied to a target table, for example your lakehouse, to keeo it up to date with the latest state from the source.\n",
    "\n",
    "  CDC is also closely related to how we handle slowly changing dimensions, or SCDs, which define how histroical changes are tracked and stored in your target\n",
    "\n",
    "    SCD Type 1 :- Overwrites existing data (no history tracking)\n",
    "    SCD Type 2 :- Tracks historical changes by storing previous versions of records\n",
    "\n",
    "  \n",
    "  Using APPLY CHANGES INTO to perform SCD in Declarative pipelines(INSERTs and UPDATEs are implicitly implemented using the KEY(s), no coding required)\n",
    "\n",
    "APPLY CHANGES INTO target_table\n",
    "FROM STREAM source_table\n",
    "KEYS (column) --The column(s) that uniquely identify a row in the source and target table(primary )\n",
    "APPLY AS DELETE WHEN operation = \"DELETE\" --Specifies whena cdc event should be treated asa DELETE rather thananupsert\n",
    "SEQUENCE BY Column_name -- specifies the logical order of cdc events in spurce data\n",
    "COLUMNS * EXCEPT (operation) -- specifes a subset of columns to include in the target\n",
    "STORED AS SCD TYPE 2; -- whether to store records as SCD TYPE 1 (default) or TYPE 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8416c090-8e4b-459c-9ec9-bd853087f5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "REPO\n",
    "\n",
    "Git:\n",
    "  new repository (https://github.com/mail4cirus/databricks_training_2025.git)\n",
    "\n",
    "\n",
    "\n",
    "Databricks:\n",
    "  create git folder using git url(workspace -> user -> user@id -> create -> git folder)\n",
    "  integrate (databricks + git) (settings -> linked accounts => github -> configure with the branch name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7142c634-de48-4417-b71d-da946b6a18cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Streaming\n",
    "***********\n",
    "1. Structure Streaming"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
